# 一、切片

## 1、FileInputFormat的切片策略（默认）

``` java
public List<InputSplit> getSplits(JobContext job) throws IOException {
StopWatch sw = new StopWatch().start();
// minSize从mapreduce.input.fileinputformat.split.minsize和1之间对比，取最大值
long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));
// 读取mapreduce.input.fileinputformat.split.maxsize，如果没有设置使用Long.MaxValue作为默认值
    long maxSize = getMaxSplitSize(job);
// generate splits
List<InputSplit> splits = new ArrayList<InputSplit>();
// 获取当前Job输入目录中所有文件的状态(元数据)
List<FileStatus> files = listStatus(job);
// 以文件为单位进行切片
    for (FileStatus file: files) {
      Path path = file.getPath();
      long length = file.getLen();
      if (length != 0) {
        BlockLocation[] blkLocations;
        if (file instanceof LocatedFileStatus) {
          blkLocations = ((LocatedFileStatus) file).getBlockLocations();
        } else {
          FileSystem fs = path.getFileSystem(job.getConfiguration());
          blkLocations = fs.getFileBlockLocations(file, 0, length);
        }
      // 判断当前文件是否可切，如果可切，切片
        if (isSplitable(job, path)) {
          long blockSize = file.getBlockSize();
          long splitSize = computeSplitSize(blockSize, minSize, maxSize);
           // 声明待切部分数据的余量
          long bytesRemaining = length;
          // 如果 待切部分 / 片大小  > 1.1，先切去一片，再判断
          while (((double) bytesRemaining)/splitSize > SPLIT_SLOP) {
            int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);
            splits.add(makeSplit(path, length-bytesRemaining, splitSize,
                        blkLocations[blkIndex].getHosts(),
                        blkLocations[blkIndex].getCachedHosts()));
            bytesRemaining -= splitSize;
          }
// 否则，将剩余部分整个作为1片。 最后一片有可能超过片大小，但是不超过其1.1倍
          if (bytesRemaining != 0) {
            int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);
            splits.add(makeSplit(path, length-bytesRemaining, bytesRemaining,
                       blkLocations[blkIndex].getHosts(),
                       blkLocations[blkIndex].getCachedHosts()));
          }
        } else { // not splitable
         // 如果不可切，整个文件作为1片！
          splits.add(makeSplit(path, 0, length, blkLocations[0].getHosts(),
                      blkLocations[0].getCachedHosts()));
        }
      } else { 
    //Create empty hosts array for zero length files
    // 如果文件是个空文件，创建一个切片对象，这个切片从当前文件的0offset起，向后读取0个字节
        splits.add(makeSplit(path, 0, length, new String[0]));
      }
    }
    // Save the number of input files for metrics/loadgen
    job.getConfiguration().setLong(NUM_INPUT_FILES, files.size());
    sw.stop();
    if (LOG.isDebugEnabled()) {
      LOG.debug("Total # of splits generated by getSplits: " + splits.size()
          + ", TimeTaken: " + sw.now(TimeUnit.MILLISECONDS));
    }
    return splits;
  }
```
### 总结：
* 1、获取当前输入目录中所有的文件
* 2、以文件为单位，如果文件是空文件，默认创建一个空的切片
* 3、如果文件不为空，尝试判断文件是否可以进行切片（除压缩文件外均可切片）
* 4、如果文件不可以切片，整个文件作为一个片
* 5、如果文件可以切片，先获取片大小（默认片大小等于块大小）
  * 循环判断 **待切部分 / 片大小 > 1.1**，如果成立，先切去一片，在判断这个不等式
* 6、剩余部分整个作为一片

## 2、从Job的配置中获取参数

``` xml
job.getConfiguration().getLong(SPLIT_MINSIZE, 1L)
```
### 指从配置文件中获取名称为SPLIT_MINSIZE的参数，如果获取到，将参数的值返回，否则就使用默认值1L

### 设置Job参数

``` java
conf.setClass(INPUT_FORMAT_CLASS_ATTR, cls, InputFormat.class);
```
* *注意：要为INPUT_FORMAT_CLASS_ATTR设置参数为cls，cls必须是InputFormat.class的子类*

## 3、TextInputFormat 处理

### TextInputFormat判断文件是否可以进行切片
``` java
    @Override
    protected boolean isSplitable(JobContext context, Path file) {
    // 根据文件的后缀名获取文件使用的相关的压缩格式
        final CompressionCodec codec = new CompressionCodecFactory(context.getConfiguration()).getCodec(file);
    // 如果文件不是一个压缩类型的文件，默认都可以切片
        if (null == codec) {
            return true;
        }
    //否则判断是否是一个可以切片的压缩格式，默认只有Bzip2压缩格式可切片
        return codec instanceof SplittableCompressionCodec;
    }
```

## 4、片大小的计算
``` java
long splitSize = computeSplitSize(blockSize, minSize, maxSize);
protected long computeSplitSize(long blockSize, long minSize,long maxSize) {
    return Math.max(minSize, Math.min(maxSize, blockSize));
}
```
blockSize: 块大小
minSize: minSize从mapreduce.input.fileinputformat.split.minsize和1之间对比，取最大值
maxSize: 读取mapreduce.input.fileinputformat.split.maxsize，如果没有设置使用Long.MaxValue作为默认值

> *注意：默认的片大小就是文件的块大小*

文件的块大小默认为128M，默认每片就是128M
调节片大小 > 块大小：配置 mapreduce.input.fileinputformat.split.minsize > 128M
调节片大小 < 块大小：配置 mapreduce.input.fileinputformat.split.maxsize < 128M

> 理论上来说：如果文件的数据量是一定的话，片越大，切片数量少，启动的MapTask少，Map阶段运算慢.片越小，切片数量多，启动的MapTask多，Map阶段运算快

## 5、片和块的关系

### 片（InputSplit）：
> 在计算MR程序时，才会切片。片在运行程序时，临时将文件从逻辑上划分为若干部分
> 使用的输入格式不同，切片的方式不同，切片的数量也不同
> 每片的数据最终也是以块的形式存储在HDFS

### 块（block）：
>  在向HDFS写文件时，文件中的内容以块为单位存储,块是实际的物理存在

### 提醒：**片大小最好等于块大小**
* 将片大小设置和块大小一致，可以最大限度减少因为切片带来的磁盘IO和网络IO
  * 原因：MR计算框架速度慢的原因在于在执行MR时，会发生频繁的磁盘IO和网络IO，将片大小设置成块大小可以起到优化MR的作用。

---

# 二、常见的输入格式

## 1、TextInputFormat

* 常用于输入目录中全部是文本文件的情况。
  * 切片策略：默认的切片策略
  * RecordReader：LineRecordReader,一次处理一行，将一行内容的偏移量作为key（类型：LongWritable），一行内容作为value（类型：Text）

## 2、NlineInputFormat

* 作用类似于TextInputFormat，但是可以自定义一次处理的行数
  * 切片策略：读取配置中mapreduce.input.lineinputformat.linespermap，默认为1，以文件为单位，切片每此参数行作为1片
  * RecordReader:  LineRecordReader,一次处理一行，将一行内容的偏移量作为key（类型：LongWritable），一行内容作为value（类型：Text）

## 3、KeyValueTextInputFormat

* 针对文本文件！使用分割字符，将每一行分割为key和value，如果没有找到分隔符，当前行的内容作为key，value为空串。默认分隔符为\t。
* 可以通过参数mapreduce.input.keyvaluelinerecordreader.key.value.separator指定分隔符。
  * 切片策略：默认的切片策略
  * RecordReader：KeyValueLineRecordReader，key和value均是Text类型

## 4、CombineTextInputFormat

* 改变了传统的切片方式，将多个小文件，划分到一个切片中。适合小文件过多的场景
* 切片策略：先确定片的最大值maxSize，maxSize可以通过参数mapreduce.input.fileinputformat.split.maxsize设置
  * ConbineTextInputFormat的切片流程
    * a、以文件为单位，将每个文件划分为若干part
      * ①判断文件的待切部分的大小 <=  maxSize,整个待切部分作为1part
      * ②maxsize < 文件的待切部分的大小 <= 2* maxSize,将整个待切部分均分为2part
      * ③文件的待切部分的大小 > 2* maxSize,先切去maxSize大小，作为1部分，剩余待切部分继续判断
    * b、将之前切分的若干part进行累加，累加后一旦累加的大小超过 maxSize，累加的部分作为1片

---

# 三、关键设置

## 如何设置MapTask的数量
MapTask的数量，人为设置是无效的！只能通过切片方式来设置！
MapTask只取决于切片数

---

# 四、Job提交流程阶段总结

## 1、准备阶段
运行Job.waitForCompletion(),先使用JobSubmitter提交Job，在提交之前，会在Job的作业目录中生成以下信息：
* job.split:  当前Job的切片信息，有几个切片对象
* job.splitmetainfo: 切片对象的属性信息
* job.xml: job所有的属性配置

## 2、提交阶段
本地模式：
* 通过LocalJobRunner提交MR程序的Job
  * 创建一个LocalJobRunner.Job(),然后通过Job.start()启动作业
  
Map阶段：采用线程池提交多个MapTaskRunner线程
* 每个MapTaskRunable线程上，实例化一个MapTask对象
* 每个MapTask对象，实例化一个Mapper，然后通过Mapper.run()开始计算
* 线程运行结束，会在线程的作业目录中生成fil.out文件，保存《MapTask输出的所有key-value
* 阶段定义：
  * 如果有reduceTask，MapTask运行期间，分为map和sort
  * 如果没有reduceTask，MapTask运行期间只有map
    * map：使用RecordReader将切片中的数据读入到Mapper.map()------>context.writer(key, value)

Reduce阶段：采用线程池提交多个ReduceTaskRunable线程
* 每个ReduceTaskRunable线程上，实例化一个ReduceTask对象
* 每个ReduceTask对象，实例化一个Reduce，然后通过reducer.run()开始计算
* 线程运行结束，会在输出目录中生成part-r-000x文件，保存ReduceTask输出的所有的key-value
* 阶段定义：
  * copy：使用shuffle线程复制MapTask指定分区的数据
  * sort：将复制的所有分区的数据汇总后进行排序
  * reduce：对排好序的数据进行合并汇总

YARN上运行：待续。

---